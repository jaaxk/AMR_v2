WARNING:root:Perform single sequence classification (no additional features)...
--- Logging error ---
Traceback (most recent call last):
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 1085, in emit
    msg = self.format(record)
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 929, in format
    return fmt.format(record)
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 668, in format
    record.message = record.getMessage()
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 373, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "../train.py", line 390, in <module>
    train()
  File "../train.py", line 299, in train
    train_dataset = SupervisedDataset(tokenizer=tokenizer,
  File "../train.py", line 169, in __init__
    logging.warning("Number of labels:", self.num_labels)
Message: 'Number of labels:'
Arguments: (2,)
WARNING:root:Perform single sequence classification (no additional features)...
--- Logging error ---
Traceback (most recent call last):
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 1085, in emit
    msg = self.format(record)
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 929, in format
    return fmt.format(record)
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 668, in format
    record.message = record.getMessage()
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 373, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "../train.py", line 390, in <module>
    train()
  File "../train.py", line 302, in train
    val_dataset = SupervisedDataset(tokenizer=tokenizer,
  File "../train.py", line 169, in __init__
    logging.warning("Number of labels:", self.num_labels)
Message: 'Number of labels:'
Arguments: (2,)
WARNING:root:Perform single sequence classification (no additional features)...
--- Logging error ---
Traceback (most recent call last):
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 1085, in emit
    msg = self.format(record)
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 929, in format
    return fmt.format(record)
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 668, in format
    record.message = record.getMessage()
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 373, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "../train.py", line 390, in <module>
    train()
  File "../train.py", line 305, in train
    test_dataset = SupervisedDataset(tokenizer=tokenizer,
  File "../train.py", line 169, in __init__
    logging.warning("Number of labels:", self.num_labels)
Message: 'Number of labels:'
Arguments: (2,)
Initializing CUSTOM BertForSequenceClassification
/gpfs/scratch/jvaska/cache/modules/transformers_modules/bacteria_model/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Dropout layer: Dropout(p=0.1, inplace=False)
Dropout probability: 0.1
Some weights of the model checkpoint at ../pretrained_models/bacteria_model were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../pretrained_models/bacteria_model and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.2.bias', 'classifier.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 65,882
  Num Epochs = 4
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 4
  Total optimization steps = 2,056
  Number of trainable parameters = 117,167,234
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
 49%|████▊     | 1000/2056 [03:08<03:18,  5.33it/s]***** Running Evaluation *****
{'loss': 0.6883, 'learning_rate': 3e-06, 'epoch': 0.19}
{'loss': 0.6124, 'learning_rate': 5.940000000000001e-06, 'epoch': 0.39}
{'loss': 0.4726, 'learning_rate': 8.939999999999999e-06, 'epoch': 0.58}
{'loss': 0.387, 'learning_rate': 1.1940000000000001e-05, 'epoch': 0.78}
{'loss': 0.3582, 'learning_rate': 1.4940000000000001e-05, 'epoch': 0.97}
{'loss': 0.3353, 'learning_rate': 1.794e-05, 'epoch': 1.17}
{'loss': 0.328, 'learning_rate': 2.0909999999999998e-05, 'epoch': 1.36}
{'loss': 0.3289, 'learning_rate': 2.3910000000000003e-05, 'epoch': 1.55}
{'loss': 0.3231, 'learning_rate': 2.691e-05, 'epoch': 1.75}
{'loss': 0.3261, 'learning_rate': 2.991e-05, 'epoch': 1.94}
  Num examples = 5295
  Batch size = 16
 49%|████▊     | 1000/2056 [03:10<03:18,  5.33iSaving model checkpoint to ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/checkpoint-1000
Configuration saved in ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/checkpoint-1000/config.json
{'eval_loss': 0.27899137139320374, 'eval_accuracy': 0.9031161473087819, 'eval_f1': 0.9016522947435833, 'eval_matthews_correlation': 0.8060356170531665, 'eval_precision': 0.8992030969251719, 'eval_recall': 0.9068689728126406, 'eval_confusion_matrix': [[2714, 358], [155, 2068]], 'eval_runtime': 1.5529, 'eval_samples_per_second': 3409.678, 'eval_steps_per_second': 53.447, 'epoch': 1.94}
Model weights saved in ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/checkpoint-1000/pytorch_model.bin
tokenizer config file saved in ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/checkpoint-1000/tokenizer_config.json
Special tokens file saved in ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/checkpoint-1000/special_tokens_map.json
 97%|█████████▋| 2000/2056 [06:19<00:10,  5.32it/s]***** Running Evaluation *****
{'loss': 0.3058, 'learning_rate': 2.7244318181818186e-05, 'epoch': 2.14}
{'loss': 0.2959, 'learning_rate': 2.4403409090909094e-05, 'epoch': 2.33}
{'loss': 0.2997, 'learning_rate': 2.15625e-05, 'epoch': 2.53}
{'loss': 0.2905, 'learning_rate': 1.8721590909090907e-05, 'epoch': 2.72}
{'loss': 0.2815, 'learning_rate': 1.590909090909091e-05, 'epoch': 2.91}
{'loss': 0.2884, 'learning_rate': 1.3068181818181819e-05, 'epoch': 3.11}
{'loss': 0.2758, 'learning_rate': 1.0227272727272727e-05, 'epoch': 3.3}
{'loss': 0.2702, 'learning_rate': 7.386363636363636e-06, 'epoch': 3.5}
{'loss': 0.2658, 'learning_rate': 4.5454545454545455e-06, 'epoch': 3.69}
{'loss': 0.2708, 'learning_rate': 1.7045454545454546e-06, 'epoch': 3.89}
  Num examples = 5295
  Batch size = 16
 97%|█████████▋| 2000/2056 [06:21<00:10,  5.32iSaving model checkpoint to ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/checkpoint-2000
Configuration saved in ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/checkpoint-2000/config.json
{'eval_loss': 0.24536238610744476, 'eval_accuracy': 0.9180358829084041, 'eval_f1': 0.9161260799082465, 'eval_matthews_correlation': 0.8324228999805912, 'eval_precision': 0.9149993463391938, 'eval_recall': 0.9174270938765182, 'eval_confusion_matrix': [[2830, 242], [192, 2031]], 'eval_runtime': 1.5512, 'eval_samples_per_second': 3413.583, 'eval_steps_per_second': 53.508, 'epoch': 3.89}
Model weights saved in ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/checkpoint-2000/pytorch_model.bin
tokenizer config file saved in ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/checkpoint-2000/tokenizer_config.json
Special tokens file saved in ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/checkpoint-2000/special_tokens_map.json
100%|██████████| 2056/2056 [06:32<00:00,  5.32it/s]

Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/checkpoint-2000 (score: 0.9161260799082465).
100%|██████████| 2056/2056 [06:32<00:00,  5.23it/s]
{'train_runtime': 393.0094, 'train_samples_per_second': 670.539, 'train_steps_per_second': 5.231, 'train_loss': 0.34823544016144153, 'epoch': 3.99}
***** Running Evaluation *****
  Num examples = 5295
  Batch size = 16
100%|██████████| 83/83 [00:01<00:00, 50.07it/s]
Configuration saved in ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/best/config.json
Model weights saved in ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/best/pytorch_model.bin
tokenizer config file saved in ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/best/tokenizer_config.json
Special tokens file saved in ../finetuned_models/per_species_models_v2/streptococcus_pneumoniae/best/special_tokens_map.json
