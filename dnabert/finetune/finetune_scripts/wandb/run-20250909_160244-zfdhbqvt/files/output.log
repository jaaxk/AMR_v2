WARNING:root:Perform single sequence classification (no additional features)...
--- Logging error ---
Traceback (most recent call last):
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 1085, in emit
    msg = self.format(record)
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 929, in format
    return fmt.format(record)
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 668, in format
    record.message = record.getMessage()
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 373, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "../train.py", line 390, in <module>
    train()
  File "../train.py", line 299, in train
    train_dataset = SupervisedDataset(tokenizer=tokenizer,
  File "../train.py", line 169, in __init__
    logging.warning("Number of labels:", self.num_labels)
Message: 'Number of labels:'
Arguments: (2,)
WARNING:root:Perform single sequence classification (no additional features)...
--- Logging error ---
Traceback (most recent call last):
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 1085, in emit
    msg = self.format(record)
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 929, in format
    return fmt.format(record)
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 668, in format
    record.message = record.getMessage()
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 373, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "../train.py", line 390, in <module>
    train()
  File "../train.py", line 302, in train
    val_dataset = SupervisedDataset(tokenizer=tokenizer,
  File "../train.py", line 169, in __init__
    logging.warning("Number of labels:", self.num_labels)
Message: 'Number of labels:'
Arguments: (2,)
WARNING:root:Perform single sequence classification (no additional features)...
--- Logging error ---
Traceback (most recent call last):
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 1085, in emit
    msg = self.format(record)
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 929, in format
    return fmt.format(record)
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 668, in format
    record.message = record.getMessage()
  File "/gpfs/scratch/jvaska/envs/dna/lib/python3.8/logging/__init__.py", line 373, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "../train.py", line 390, in <module>
    train()
  File "../train.py", line 305, in train
    test_dataset = SupervisedDataset(tokenizer=tokenizer,
  File "../train.py", line 169, in __init__
    logging.warning("Number of labels:", self.num_labels)
Message: 'Number of labels:'
Arguments: (2,)
Initializing CUSTOM BertForSequenceClassification
/gpfs/scratch/jvaska/cache/modules/transformers_modules/bacteria_model/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Dropout layer: Dropout(p=0.1, inplace=False)
Dropout probability: 0.1
Some weights of the model checkpoint at ../pretrained_models/bacteria_model were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../pretrained_models/bacteria_model and are newly initialized: ['classifier.0.bias', 'bert.pooler.dense.weight', 'classifier.2.weight', 'classifier.2.bias', 'bert.pooler.dense.bias', 'classifier.0.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 30,799
  Num Epochs = 4
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 4
  Total optimization steps = 960
  Number of trainable parameters = 117,167,234
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
100%|██████████| 960/960 [03:01<00:00,  5.30it/s]
{'loss': 0.688, 'learning_rate': 2.9700000000000004e-06, 'epoch': 0.42}
{'loss': 0.6396, 'learning_rate': 5.9700000000000004e-06, 'epoch': 0.83}
{'loss': 0.5391, 'learning_rate': 8.97e-06, 'epoch': 1.25}
{'loss': 0.4788, 'learning_rate': 1.1940000000000001e-05, 'epoch': 1.66}
{'loss': 0.4409, 'learning_rate': 1.488e-05, 'epoch': 2.08}
{'loss': 0.3758, 'learning_rate': 1.7879999999999998e-05, 'epoch': 2.49}
{'loss': 0.3334, 'learning_rate': 2.085e-05, 'epoch': 2.91}
{'loss': 0.2932, 'learning_rate': 2.385e-05, 'epoch': 3.32}
{'loss': 0.2634, 'learning_rate': 2.6850000000000002e-05, 'epoch': 3.74}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 960/960 [03:01<00:00,  5.29it/s]
{'train_runtime': 181.5212, 'train_samples_per_second': 678.687, 'train_steps_per_second': 5.289, 'train_loss': 0.4374019424120585, 'epoch': 3.99}
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
100%|██████████| 79/79 [01:14<00:00,  1.05it/s]
Configuration saved in ../finetuned_models/per_species_models_v2/staphylococcus_aureus/best/config.json
Model weights saved in ../finetuned_models/per_species_models_v2/staphylococcus_aureus/best/pytorch_model.bin
tokenizer config file saved in ../finetuned_models/per_species_models_v2/staphylococcus_aureus/best/tokenizer_config.json
Special tokens file saved in ../finetuned_models/per_species_models_v2/staphylococcus_aureus/best/special_tokens_map.json
