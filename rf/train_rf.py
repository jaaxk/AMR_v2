"""
Train a RF model on the dataset(s) generated by dnabert/inference/inference.py
    - Should be located at ../dnabert/inference/outputs/rf_datasets/{MODEL_NAME}/{grouping}/{train} by default
    - Trains RF model on DEV portion of TRAIN set 
        - This is because DNABERT was trained on train portion, so we cant use this poriton for training RF

If --eval is true, will evaluate models on TEST PORTION OF TRAIN SET, not independent validation test set from CAMDA site
"""


# lists/mappings
species_list = [
    'neisseria_gonorrhoeae', 
    'staphylococcus_aureus', 
    'streptococcus_pneumoniae', 
    'salmonella_enterica', 
    'klebsiella_pneumoniae', 
    'escherichia_coli', 
    'pseudomonas_aeruginosa', 
    'acinetobacter_baumannii', 
    'campylobacter_jejuni' 
]

antibiotic_to_species ={
    'GEN': ['klebsiella_pneumoniae', 'escherichia_coli', 'salmonella_enterica'],
    'ERY': ['streptococcus_pneumoniae', 'staphylococcus_aureus'],
    'CAZ': ['pseudomonas_aeruginosa', 'acinetobacter_baumannii'],
    'TET': ['neisseria_gonorrhoeae', 'campylobacter_jejuni'],
}


# imports
import argparse
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import joblib
import os
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import json
from tqdm import tqdm
import numpy as np
import csv
import io
from contextlib import redirect_stdout


# methods

def filter_feature_type(X, feature_type, top_n_mi_score=None):
    """ Remove hit_count or pred_resistant columns based on feature_type,
    as well as other unnecessary columns like accession, species, antibiotic, ground_truth_phenotype,
    to be used for X dataset (features) """

    X = X.drop(['accession', 'species', 'antibiotic', 'ground_truth_phenotype'], axis=1) # we dont need species and antibiotic if were doing per-species models (all the same)

    if feature_type == 'dnabert':
        cols_to_keep = [col for col in X.columns if 'pred_resistant' in col]
        X = X[cols_to_keep]
    elif feature_type == 'hits':
        cols_to_keep = [col for col in X.columns if 'hit_count' in col]
        X = X[cols_to_keep]
    elif feature_type == 'both':
        return X
    else:
        raise ValueError(f"Invalid feature type: {feature_type}")

    return X


def filter_top_n_mi(X, species, models_dir, y=None, top_n_mi_score=None):
    if top_n_mi_score is None:
        return X
    else:
        if y is not None:
            from sklearn.feature_selection import mutual_info_classif
            mi_scores = mutual_info_classif(X, y, discrete_features='auto')
            mi_df = pd.DataFrame({
                "feature": X.columns,
                "mi_score": mi_scores
            }).sort_values(by="mi_score", ascending=False)

            features_to_keep = mi_df.head(top_n_mi_score).feature
            X = X[features_to_keep]
            #save features so we can load them during inference
            os.makedirs(os.path.join(models_dir, 'mi_features'), exist_ok=True)
            path_to_save = os.path.join(models_dir, 'mi_features', f'{species}_mi_features.txt')
            with open(path_to_save, 'w') as f:
                for feature in features_to_keep:
                    f.write(f"{feature}\n")
        else: #if we dont pass y (during inference), we must have already saved features during training
            #load features
            path_to_load = os.path.join(models_dir, 'mi_features', f'{species}_mi_features.txt')
            with open(path_to_load, 'r') as f:
                features_to_keep = [line.strip() for line in f]
            X = X[features_to_keep]
        

        return X



    

    return X

def train(args):

    if args.grouping == 'per_species':
        for species in species_list:
            if args.train_on == 'test': #dont train on this, save this for independent eval (unless submitting for final eval)
                df = pd.read_csv(os.path.join(args.dataset_dir, species, f"{species}_full_rf_dataset_test.csv"))
            elif args.train_on == 'dev': #recommended if using DNABERT features
                df = pd.read_csv(os.path.join(args.dataset_dir, species, f"{species}_full_rf_dataset_dev.csv"))
            elif args.train_on == 'train': #dont do this if using DNABERT features
                df = pd.read_csv(os.path.join(args.dataset_dir, species, f"{species}_full_rf_dataset_train.csv"))
            elif args.train_on == 'train_dev': #dont do this if using DNABERT features
                df = pd.concat([pd.read_csv(os.path.join(args.dataset_dir, species, f"{species}_full_rf_dataset_train.csv")), pd.read_csv(os.path.join(args.dataset_dir, species, f"{species}_full_rf_dataset_dev.csv"))])
            elif args.train_on == 'test_dev': #train on test and dev set (for final CAMDA eval)
                df = pd.concat([pd.read_csv(os.path.join(args.dataset_dir, species, f"{species}_full_rf_dataset_test.csv")), pd.read_csv(os.path.join(args.dataset_dir, species, f"{species}_full_rf_dataset_dev.csv"))])
            else:
                raise ValueError(f"Invalid train_on value: {args.train_on}")

            print(f"Training RF model for {species}")

            X = filter_feature_type(df, args.feature_type)
            y = df['ground_truth_phenotype']
            X = filter_top_n_mi(X=X, y=y, species=species, models_dir=args.out_dir, top_n_mi_score=args.top_n_mi_score)

            print(args.feature_type)
            print(X.columns)
            assert len(X) == len(y)

            model = RandomForestClassifier(n_estimators=args.n_estimators)
            model.fit(X, y)

            #save model
            model_path = os.path.join(args.out_dir, f"{species}_rf_model.joblib")
            joblib.dump(model, model_path)

    else:
        raise NotImplementedError(f"Grouping {args.grouping} not implemented")




def eval(args):
    """Evaluate RF models on TEST PORTION OF TRAIN SET, not independent validation test set from CAMDA site"""


    metrics = {
    'accuracy': {},
        'precision': {},
        'recall': {},
        'f1': {},
        'aucroc': {},
        }

    for species in species_list:
        model_path = os.path.join(args.out_dir, f"{species}_rf_model.joblib")
        model = joblib.load(model_path)
        test_df = pd.read_csv(os.path.join(args.dataset_dir, species, f"{species}_full_rf_dataset_test.csv"))
        X = test_df
        X = filter_feature_type(X, args.feature_type)
        X = filter_top_n_mi(X=X, species=species, models_dir=args.out_dir, top_n_mi_score=args.top_n_mi_score)
            
        preds = model.predict(X)
        truth = test_df['ground_truth_phenotype'].values
        assert len(preds) == len(truth)
        metrics['accuracy'][species] = accuracy_score(truth, preds)
        metrics['precision'][species] = precision_score(truth, preds)
        metrics['recall'][species] = recall_score(truth, preds)
        metrics['f1'][species] = f1_score(truth, preds)
        metrics['aucroc'][species] = roc_auc_score(truth, preds)

    
    # report metrics
    for metric_name, metric_dict in metrics.items():
        print(f"Mean {metric_name}: {np.mean(list(metric_dict.values()))}")
        print()
        for species in species_list:
            print(f"{species}: {metric_dict[species]}")
        print()

    # save metrics to json
    results_dir = os.path.join('eval_results', args.model_name)
    os.makedirs(results_dir, exist_ok=True)
    with open(os.path.join(results_dir, 'eval_results.json'), 'w') as f:
        json.dump(metrics, f)

    # save args to results dir
    with open(os.path.join(results_dir, 'args.json'), 'w') as f:
        json.dump(args.__dict__, f)

    if not os.path.exists('eval_results/rf_models.csv'):

        #write results to common CSV
        with open('eval_results/rf_models.csv', 'w') as f:
            writer = csv.writer(f)
            #columns:
            writer.writerow(['accuracy', 'model_name', 'train_on', 'feature_type', 'n_estimators', \
            'max_depth', 'top_n_mi_score', 'recall', 'precision', 'f1', 'aucroc', 'neisseria_gonorrhoeae_acc', \
            'staphylococcus_aureus_acc', 'streptococcus_pneumoniae_acc', 'salmonella_enterica_acc', 'klebsiella_pneumoniae_acc', \
            'escherichia_coli_acc', 'pseudomonas_aeruginosa_acc', 'acinetobacter_baumannii_acc', 'campylobacter_jejuni_acc', \
            'TET_acc', 'GEN_acc', 'ERY_acc', 'CAZ_acc'])

    with open('eval_results/rf_models.csv', 'a') as f:
        writer = csv.writer(f)

        accuracy = np.mean(list(metrics['accuracy'].values()))
        recall = np.mean(list(metrics['recall'].values()))
        precision = np.mean(list(metrics['precision'].values()))
        f1 = np.mean(list(metrics['f1'].values()))
        aucroc = np.mean(list(metrics['aucroc'].values()))
        neisseria_gonorrhoeae_acc = metrics['accuracy']['neisseria_gonorrhoeae']
        staphylococcus_aureus_acc = metrics['accuracy']['staphylococcus_aureus']
        streptococcus_pneumoniae_acc = metrics['accuracy']['streptococcus_pneumoniae']
        salmonella_enterica_acc = metrics['accuracy']['salmonella_enterica']
        klebsiella_pneumoniae_acc = metrics['accuracy']['klebsiella_pneumoniae']
        escherichia_coli_acc = metrics['accuracy']['escherichia_coli']
        pseudomonas_aeruginosa_acc = metrics['accuracy']['pseudomonas_aeruginosa']
        acinetobacter_baumannii_acc = metrics['accuracy']['acinetobacter_baumannii']
        campylobacter_jejuni_acc = metrics['accuracy']['campylobacter_jejuni']
        TET_acc = np.mean([metrics['accuracy'][species] for species in antibiotic_to_species['TET']])
        GEN_acc = np.mean([metrics['accuracy'][species] for species in antibiotic_to_species['GEN']])
        ERY_acc = np.mean([metrics['accuracy'][species] for species in antibiotic_to_species['ERY']])
        CAZ_acc = np.mean([metrics['accuracy'][species] for species in antibiotic_to_species['CAZ']])
        
        writer.writerow([accuracy, args.model_name, args.train_on, args.feature_type, args.n_estimators, \
        args.max_depth, args.top_n_mi_score, recall, precision, f1, aucroc, neisseria_gonorrhoeae_acc, \
        staphylococcus_aureus_acc, streptococcus_pneumoniae_acc, salmonella_enterica_acc, klebsiella_pneumoniae_acc,\
        escherichia_coli_acc, pseudomonas_aeruginosa_acc, acinetobacter_baumannii_acc, campylobacter_jejuni_acc, TET_acc,\
        GEN_acc, ERY_acc, CAZ_acc])
        f.flush()

    return metrics


def tune_hyperparams(args):
    """iterate through tunable hyperparams to get best model based on args.tune_metric
    writes all models tested to eval_results/rf_models.csv"""


    tree_params = {
        'n_estimators': [100, 200, 250, 300, 400],
        'max_depth': [20, 30, 40, 50, 60],
        'top_n_mi_score': [100, 200, 300, 400],
        'feature_type': ['dnabert', 'hits']
    }

    print(f"Tuning hyperparams with parameters: {tree_params}...")

    if not os.path.exists('eval_results/rf_models.csv'):
        with open('eval_results/rf_models.csv', 'w') as f:
            writer = csv.writer(f)
            #columns:
            writer.writerow(['accuracy', 'model_name', 'train_on', 'feature_type', 'n_estimators', \
            'max_depth', 'top_n_mi_score', 'recall', 'precision', 'f1', 'aucroc', 'neisseria_gonorrhoeae_acc', \
            'staphylococcus_aureus_acc', 'streptococcus_pneumoniae_acc', 'salmonella_enterica_acc', 'klebsiella_pneumoniae_acc', \
            'escherichia_coli_acc', 'pseudomonas_aeruginosa_acc', 'acinetobacter_baumannii_acc', 'campylobacter_jejuni_acc', \
            'TET_acc', 'GEN_acc', 'ERY_acc', 'CAZ_acc'])

    with open('eval_results/rf_models.csv', 'a') as f:
        writer = csv.writer(f)

        for n_estimators in tqdm(tree_params['n_estimators']):
            for max_depth in tqdm(tree_params['max_depth'], leave=False):
                for top_n_mi_score in tqdm(tree_params['top_n_mi_score'], leave=False):
                    for feature_type in tqdm(tree_params['feature_type'], leave=False):
                        args.feature_type = feature_type
                        args.n_estimators = n_estimators
                        args.max_depth = max_depth
                        args.top_n_mi_score = top_n_mi_score
                        with io.StringIO() as buf, redirect_stdout(buf): #hide outputs while tuning
                            train(args)
                            metrics = eval(args)

                    
            
            
        


    

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset_dir', type=str, help='Path to directory containing datasets formatted by dnabert/inference/inference.py', default='../dnabert/inference/outputs/rf_datasets/full_model_species_only_v2/per_species/train')
    parser.add_argument('--grouping', type=str, choices=['full', 'per_antibiotic', 'per_species'], help='Grouping for models (per_species=9 models, per_antibiotic=4 models, full=1 model)', default='per_species')
    parser.add_argument('--model_name', type=str, help='Name of DNABERT model used and info about this run', default=None)
    parser.add_argument('--out_dir', type=str, help='Directory to save models', default=None)
    parser.add_argument('--train_on', type=str, choices=['test', 'dev', 'test_dev', 'train', 'train_dev'], help='Which portion of TRAIN set to train on. If using DNABERT, it is NOT recommended to train on train portion of train set. Eval is always on TEST', default='dev')
    parser.add_argument('--feature_type', type=str, choices=['dnabert', 'hits', 'both'], help='Whether to use DNABERT features, hits, or both', default='both')
    parser.add_argument('--eval', type=bool, help='Whether to evaluate models', default=False)

    parser.add_argument('--tune_hyperparams', type=bool, help='Whether to tune hyperparameters', default=False)
    parser.add_argument('--tune_metric', type=str, choices=['accuracy', 'precision', 'recall', 'f1', 'aucroc'], help='Metric to tune hyperparameters on', default='accuracy')
    #tunable hyperparams
    parser.add_argument('--n_estimators', type=int, help='Number of trees in the random forest', default=100)
    parser.add_argument('--max_depth', type=int, help='Maximum depth of the trees', default=None)
    parser.add_argument('--top_n_mi_score', type=int, help='Number of features to keep based on mutual information score', default=None)

    args = parser.parse_args()
    if args.out_dir is None:
        args.out_dir = os.path.join('models', 'rf', args.grouping, args.model_name)
    os.makedirs(args.out_dir, exist_ok=True)

    if args.tune_hyperparams:
        tune_hyperparams(args)
    else:
        train(args)
        if args.eval:
            eval(args)


if __name__ == '__main__':
    main()

